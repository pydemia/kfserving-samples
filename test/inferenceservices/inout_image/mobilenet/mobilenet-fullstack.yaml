apiVersion: "serving.kubeflow.org/v1alpha2"
kind: "InferenceService"
metadata:
  controller-tools.k8s.io: "1.0"
  name: mobilenet-fullstack
spec:
  default:
    predictor:
      # minReplicas: 1
      # maxReplicas: 3
      serviceAccountName: yjkim-private-deployer-s3
      tensorflow:
        storageUri: "s3://yjkim-models/kfserving/mobilenet/predictor/mobilenet_saved_model"
        runtimeVersion: "1.14.0"
        # resources:
        #   requests:
        #     cpu: 200m
        #     memory: 1Gi                        
        #   limits:
        #     cpu: 400m
        #     memory: 1.2Gi
    transformer:
      # minReplicas: 1
      # maxReplicas: 3
      serviceAccountName: yjkim-private-deployer-gs
      custom:
        imagePullSecrets:
        - name: pydemia-docker-private-key
        # - name: yjkim-kube-admin-sa-gcr-private-key
        container:
          image: docker.io/pydemia/mobilenet_transformer:tf1.15.2-0.2.0
          name: kfserving-container
          # resources:
          #   requests:
          #     cpu: 100m
          #     memory: 1Gi
          #   limits:
          #     cpu: 300m
          #     memory: 1Gi
    explainer:
      # serviceAccountName: s3-private-sa
      # custom:
      #   imagePullSecrets:
      #   - name: docker-secret-key
      #   container:
      #     image: docker.io/pydemia/alibiexplainer:v0.4.0
      #     name: kfserving-container
      #     env:
      #     - name: MODEL_NAME
      #       value: microorganism
      #     - name: MODEL_BASE_PATH
      #       value: /mnt/models
      #     - name: STORAGE_URI
      #       value: s3://yjkim-models/kfserving/mobilenet/explainer
      serviceAccountName: yjkim-private-deployer-s3
      # minReplicas: 1
      # maxReplicas: 1
      alibi:
        type: AnchorImages
        storageUri: "s3://yjkim-models/kfserving/mobilenet/explainer"
        runtimeVersion: "v0.3.2-predict_fn"
        config:
          batch_size: "13"
          verbose: "1"
          #n_covered_ex: "10"
          #coverage_samples: "20"
          # beam_size: "2"
        # resources:
        #   requests:
        #     cpu: 200m
        #     memory: 1Gi            
        #   limits:
        #     cpu: 500m
        #     memory: 2Gi



    # kubectl -n inference-test get inferenceservice mobilenet-fullstack
# kubectl -n inference-test describe inferenceservice mobilenet-fullstack
# kubectl -n inference-test describe deployment mobilenet-fullstack
# kubectl -n inference-test get events mobilenet-fullstack
# kubectl -n inference-test get ksvc mobilenet-fullstack-predictor-default -o yaml
# kubectl -n inference-test get ksvc mobilenet-fullstack-explainer-default -o yaml
# kubectl -n inference-test get ksvc mobilenet-fullstack-transformer-default -o yaml
# kubectl -n inference-test get kpa -o yaml
# kubectl -n inference-test get pods mobilenet-fullstack
# kubectl -n inference-test get pods mobilenet-fullstack
# kubectl -n inference-test describe deployment mobilenet-fullstack
# kubectl -n inference-test get revision mobilenet-fullstack-predictor-default-ttlpz -o yaml
# 